## Agenda

- History of Deep Learning
- Wide & Deep Networks



Cross Entropy : good for what classification?

	> Not Regression bitch [its probability of categorical classes]

Mean Squared Error

> classify & regress

Rectified Linear Unit

> Easy derivative, and does not saturate as quickly, has bounds, gradient does not just banish. ReLU should work for N-1 layers



## Cross Validation

WTF is Actually Cross validation in terms of grid search?

grid search : find best hyper parameter given every combination of unoptimized variables

-> for this I need to train/test split

Randomized Grid Search -> if you don't have the time just cut out at some time [actually do map and parallelize that shit] /

​	Olivier whatever : sklearn founder

​	you can do mmap file so you can have less ram, and you can kill it



## The Problem

Pretty much overfitting -> cuz we're working with the same data. It could easily be biased and cannot realistically determine the expected performance on new data



So what we do? : VALIDATION SET boy. But to choose a good validation sets, nested cross validation

Outer loop reselect the test fold, and inner loop do gridsearch and tune parameters.

For test fold, you're only testing that once in the end after you do your inner train/validate stuff and tuned your parameters



inner : test and select the best test parameter -> gridsearchCV stuff

outer : use the selected parameters



What is the goal of NCV?

- To estimate geenralization performance when performing hyper param tuning
- To estimate the variation in tuned hyper parems



## History of Deep Learning

### Neural Networks : where did we leave it?

​	1986 : AI Winter ends with Rumelhart, hinton -> gradient calculation for MLN

Back Proping comes in, and since 1993 its the best one

Back Propagation saved AI Winter for NN



​	BP is no longer efficient for consistently train good nn for image

cuz gradient gets smaller and it stagnates, cannot train deeper network



​	2001 : SVM and Random Forest get traction

Second AI Winter, NN plummets

For TLP, its easy to train and performs on par with other (memorization -> memorize stuff and recall)

For Multi Layer, hard to train, and worse than other methods, researcher have difficulty reconciling expressiveness with performance : it comes out as chances not learning (generalization -> generalize features into smaller gradients, so bad representation we have will screw the learning / cuz the last two updates, not the other layers. the others are just random.)



​	2004 : Hinton secures funding from CIFAR (Candian group) based on his reputation

Hinton Rebrands : Deep Learning (market NN into DL so he can get funding)



​	2006: Hinton publishes paper on pretraining and Restricted Boltzmann Machines

​	2007: He says actually focus on pre training (auto encoding)



When we say "we don't have enough data" we usually means we dont have enough "labeled data." 

Originally we take input, encode it, decode it to reconstruct [train with shit load of unlabeled data] -> this is compression!

Then we screw decode, so we pretrain and encode, than just fking classify! [train with labeled data]



The middle layer is the core set of descriptors, which is the ideal dimensionality reduction for my shit.



​	2009 : Andrew Ng, and GPU from Hinton's lab

​	2010 : Hinton's and Ng's students go to internship -> destroy the current methods

​	2011 : Glort and Bengio investigate more systematic methods for why past deep architectures did not work

- discovers that with simple fix in type of neurons and selection of initial weight
- don't need require pretraining to get deep netowkrs and ReLU does well



ReLU and Batch Normalization

배우고 싶다. 이 모든 것을. 남는 시간에 공부를 하고 운동을 하고 나를 더 나은 인간으로 만들 고 싶다.

* SWISH : Better than ReLU (w/ or w/o batch normalization)



2012 : ImageNet competition

- Second Place was 26.2% error rate
- First Place by Hinton's Lab use convulutional network with ReLU and drop out
- 15.2%
- Fei Fei Li's lab was hosting it, and now we have image recognition on thumbnails we have better than human level.
- Deep Neural Networks for Acoustic Modeling in Speech Recognition

2013 : Ng and Google founded BrainTeam into deep learning teeam.

- run unsupervised feature creation on youtube

Hinton summarized what we learned in deep learning from the 2006 to present

- we had too small of dataset
- too slow computers
- weights were initialized wrongly
- used wrong non-linearity

this means

- use GPU, init weights for consistent gradietn magnitude, ReLu of BN where it makes sense like in early feedforward layer, lots of dropout in the final layers
- large dataset, GPU



READ : Deep Learning (2015) LeCunn, Bengio, Hinton



## Examples of Deep Learning

Take images with captions, put it in CNN put that in RNN for generating words, 